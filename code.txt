# # 1 dmv

import warnings 
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import json
import seaborn as sns

csv = pd.read_csv("sales data.csv")
ed = pd.read_excel("sales data.xlsx")
jd=pd.read_json("sales data.json")
all_data = pd.concat([csv, ed, jd], ignore_index=True)
print("Data Combined Successfully")
print("Missing Values:\n", all_data.isnull().sum())

all_data.drop_duplicates(inplace=True)
print("Duplicates Removed Successfully")
all_data

all_data['Date'] = pd.to_datetime(all_data['Date'], errors='coerce')  #trans

csv.boxplot()
sns.pairplot(ed)
plt.show()

plt.figure(figsize=(20,10))
category_counts = all_data['Rating'].value_counts()
category_counts.plot(kind='bar')
plt.title('Product Category Distribution')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

------------------------------------------------------------------------------------------------------

# # 2 dmv

df= pd.read_csv('weather.csv')
df


df.isnull().sum()


# Group by date and calculate average temperature
avg_temp = df.groupby('Date.Full')['Data.Temperature.Avg Temp'].mean().reset_index()

# Visualization
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(avg_temp['Date.Full'], avg_temp['Data.Temperature.Avg Temp'], marker='o', color='blue')
plt.title('Average Temperature Over Time')
plt.xlabel('Date')
plt.ylabel('Average Temperature (°F)')
plt.xticks(rotation=45)
plt.grid()
plt.show()


# Group by date and calculate total precipitation
total_precipitation = df.groupby('Date.Full')['Data.Precipitation'].sum().reset_index()

# Visualization
plt.figure(figsize=(12, 6))
plt.bar(total_precipitation['Date.Full'], total_precipitation['Data.Precipitation'], color='skyblue')
plt.title('Total Precipitation Over Time')
plt.xlabel('Date')
plt.ylabel('Precipitation (inches)')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.show()


# Group by city and calculate average wind speed
avg_wind_speed = df.groupby('Station.City')['Data.Wind.Speed'].mean().reset_index()

# Visualization
plt.figure(figsize=(20, 8))
plt.bar(avg_wind_speed['Station.City'], avg_wind_speed['Data.Wind.Speed'], color='orange')
plt.title('Average Wind Speed by City')
plt.xlabel('City')
plt.ylabel('Average Wind Speed (mph)')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.show()

------------------------------------------------------------------------------------------------------
# # 3 dmv

import pandas as pd


df=pd.read_csv('telecom_churn.csv')
df

# Convert 'date_of_registration' to datetime
df['date_of_registration'] = pd.to_datetime(df['date_of_registration'], errors='coerce')

# Handle missing or invalid data in 'data_used'
df['data_used'] = df['data_used'].apply(lambda x: abs(x) if x < 0 else x)

# Check for any remaining missing values
print(df.isnull().sum())

df


# Calculate tenure (years as a customer)
df['tenure_years'] = (pd.to_datetime("2024-01-01") - df['date_of_registration']).dt.days // 365

# Binning 'estimated_salary' into income brackets
df['income_bracket'] = pd.cut(df['estimated_salary'], bins=[0, 50000, 100000, 150000, 200000], labels=['Low', 'Medium', 'High', 'Very High'])

# Convert categorical variables to numeric
df = pd.get_dummies(df, columns=['telecom_partner', 'gender', 'state', 'city', 'income_bracket'], drop_first=True)


df

import matplotlib.pyplot as plt
import seaborn as sns

# Churn distribution
sns.countplot(x='churn', data=df)
plt.title('Churn Distribution')
plt.show()

# Numerical feature analysis
numerical_features = ['age', 'num_dependents', 'estimated_salary', 'calls_made', 'sms_sent', 'data_used', 'tenure_years']
for feature in numerical_features:
    plt.figure()
    sns.boxplot(x='churn', y=feature, data=df)
    plt.title(f'{feature} vs. Churn')
    plt.show()

# Categorical feature analysis (e.g., telecom_partner)
sns.countplot(x='telecom_partner_Reliance Jio', hue='churn', data=df)
plt.title('Churn by Telecom Partner')
plt.show()

------------------------------------------------------------------------------------------------------

# # dmv 4


df=pd.read_csv('Real-Estate dataset.csv')
df


# Check for missing values
print(df.isnull().sum())

# Convert binary categorical columns to 0 and 1
binary_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']
for col in binary_columns:
    df[col] = df[col].apply(lambda x: 1 if x == 'yes' else 0)

# Convert categorical column 'furnishingstatus' to numerical encoding
df = pd.get_dummies(df, columns=['furnishingstatus'], drop_first=True)


# Add 'price_per_sqft' feature
df['price_per_sqft'] = df['price'] / df['area']

# Add 'total_rooms' feature
df['total_rooms'] = df['bedrooms'] + df['bathrooms']

# Create 'luxury_index' based on amenities
df['luxury_index'] = df[['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']].sum(axis=1)


df


import seaborn as sns
import matplotlib.pyplot as plt

# Correlation matrix
plt.figure(figsize=(18,12))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Scatter plot for 'area' vs 'price'
plt.figure(figsize=(8,6))
sns.scatterplot(data=df, x='area', y='price')
plt.title("Area vs Price")
plt.show()

# Box plot for 'furnishingstatus' vs 'price'
plt.figure(figsize=(8,6))
sns.boxplot(x='furnishingstatus_semi-furnished', y='price', data=df)
plt.title("Furnishing Status vs Price")
plt.show()


from sklearn.preprocessing import StandardScaler

# List of numerical columns to scale
num_cols = ['price', 'area', 'price_per_sqft', 'total_rooms', 'luxury_index', 'bedrooms', 'bathrooms', 'stories', 'parking']

# Apply standard scaling
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])


df

------------------------------------------------------------------------------------------------------

# # 5 dmv


df=pd.read_csv('AQI Data Set.csv')
df


# Check for missing values
print(df.isnull().sum())

# Fill missing values (for this example, we'll fill with the mean of the columns)
df.fillna(df.mean(), inplace=True)

# Verify the changes
print(df.isnull().sum())
df=df.dropna()


# Convert 'Months' to datetime
df['Mounths'] = pd.to_datetime(df['Mounths'], format='%b-%y')

# Sort the DataFrame by 'Months'
df.sort_values('Mounths', inplace=True)

print(df)


import matplotlib.pyplot as plt

# Set the style for the plots
plt.style.use('seaborn-darkgrid')

# Create a figure and axis
fig, ax = plt.subplots(figsize=(14, 8))

# Plot AQI trends
ax.plot(df['Mounths'], df['AQI'], marker='o', label='AQI', color='blue')

# Plot trends for different pollutants
ax.plot(df['Mounths'], df['PM10'], marker='o', label='PM10', color='orange')
ax.plot(df['Mounths'], df['SO2'], marker='o', label='SO2', color='green')
ax.plot(df['Mounths'], df['NOx'], marker='o', label='NOx', color='red')
ax.plot(df['Mounths'], df['PM2.5'], marker='o', label='PM2.5', color='purple')

# Add labels and title
ax.set_xlabel('Mounths', fontsize=14)
ax.set_ylabel('Concentration (µg/m³)', fontsize=14)
ax.set_title('Air Quality Index (AQI) Trends and Pollutants', fontsize=16)
ax.legend()
ax.grid()

# Show the plot
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

------------------------------------------------------------------------------------------------------

# # 6 dmv

df=pd.read_csv('retail_sales_data.csv')
df


# Calculate total sales
df['total_sales'] = df['quantity'] * df['price']

# Group by shopping mall and sum the sales
sales_by_region = df.groupby('shopping_mall')['total_sales'].sum().reset_index()

# Sort by total sales in descending order
sales_by_region = sales_by_region.sort_values(by='total_sales', ascending=False)

print(sales_by_region)


import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.bar(sales_by_region['shopping_mall'], sales_by_region['total_sales'], color='skyblue')
plt.title('Total Sales Performance by Shopping Mall')
plt.xlabel('Shopping Mall')
plt.ylabel('Total Sales')
plt.xticks(rotation=45)
plt.show()


# Calculate total sales by category
sales_by_category = df.groupby('category')['total_sales'].sum().reset_index()

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(sales_by_category['category'], sales_by_category['total_sales'], color='lightgreen')
plt.title('Total Sales by Product Category')
plt.xlabel('Product Category')
plt.ylabel('Total Sales')
plt.xticks(rotation=45)
plt.show()


# Calculate total sales by payment method
sales_by_payment_method = df.groupby('payment_method')['total_sales'].sum().reset_index()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(sales_by_payment_method['total_sales'], labels=sales_by_payment_method['payment_method'], autopct='%1.1f%%', startangle=140)
plt.title('Sales Distribution by Payment Method')
plt.axis('equal')  # Equal aspect ratio
plt.show()


# Calculate total sales by gender and category
sales_by_gender_category = df.groupby(['gender', 'category'])['total_sales'].sum().unstack()

# Create a grouped bar chart
sales_by_gender_category.plot(kind='bar', figsize=(10, 6), color=['#ff9999', '#66b3ff'])
plt.title('Sales Performance by Gender and Category')
plt.xlabel('Gender')
plt.ylabel('Total Sales')
plt.xticks(rotation=0)
plt.legend(title='Product Category')
plt.show()

------------------------------------------------------------------------------------------------------

Maze ML -6
import numpy as np
import random

# Maze environment settings
maze = [
    [0, 0, 0, -1, 0],
    [0, -1, 0, -1, 0],
    [0, -1, 0, 0, 0],
    [0, 0, -1, -1, 0],
    [0, 0, 0, -1, 10],  # Goal at (4, 4)
]
start_pos = (0, 0)
goal_pos = (4, 4)
actions = ["up", "down", "left", "right"]

# Q-learning parameters
alpha = 0.1
gamma = 0.9
epsilon = 0.2
episodes = 1000
max_steps = 100

# Initialize Q-table with zeros
q_table = np.zeros((len(maze), len(maze[0]), len(actions)))

# Helper functions
def get_next_state(state, action):
    i, j = state
    if action == "up" and i > 0:
        return (i - 1, j)
    elif action == "down" and i < len(maze) - 1:
        return (i + 1, j)
    elif action == "left" and j > 0:
        return (i, j - 1)
    elif action == "right" and j < len(maze[0]) - 1:
        return (i, j + 1)
    return state  # Stay in place if move is invalid

def get_reward(state):
    return maze[state[0]][state[1]]

# Training loop
for episode in range(episodes):
    state = start_pos
    for step in range(max_steps):
        # Choose action using epsilon-greedy policy
        if random.uniform(0, 1) < epsilon:
            action = random.choice(range(len(actions)))
        else:
            action = np.argmax(q_table[state[0], state[1]])
        
        next_state = get_next_state(state, actions[action])
        reward = get_reward(next_state)

        # Q-learning formula
        best_next_action = np.argmax(q_table[next_state[0], next_state[1]])
        q_table[state[0], state[1], action] += alpha * (reward + gamma * q_table[next_state[0], next_state[1], best_next_action] - q_table[state[0], state[1], action])

        state = next_state
        
        # Check if the goal is reached
        if state == goal_pos:
            break

# Testing the trained agent
state = start_pos
path = [state]
while state != goal_pos:
    action = np.argmax(q_table[state[0], state[1]])
    state = get_next_state(state, actions[action])
    path.append(state)
print("Path to goal:", path)

------------------------------------------------------------------------------------------------------

ML 4 K-Means clustering

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the Iris dataset
#url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'
data = pd.read_csv('iris.csv')

# Drop the 'species' column for clustering, as we are clustering based on features
X = data.drop(columns=['Species'])

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# Use the elbow method to find the optimal number of clusters
wcss = []

# Test K values from 1 to 10
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)  # Inertia: WCSS for each k

# Plot the WCSS for each number of clusters
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='-')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.show()
# Assuming the optimal number of clusters is 3 (determined from the elbow plot)
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)

# Add the cluster labels to the original data
data['Cluster'] = y_kmeans
print(data.head())


# Visualize the clusters based on two features
plt.figure(figsize=(8, 5))
plt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')
plt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1], s=50, c='blue', label='Cluster 2')
plt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1], s=50, c='green', label='Cluster 3')

# Plot the centroids
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='yellow', marker='X', label='Centroids')
plt.title('K-Means Clustering of Iris Dataset')
plt.xlabel('Feature 1 (Standardized)')
plt.ylabel('Feature 2 (Standardized)')
plt.legend()
plt.show()
from sklearn.metrics import silhouette_score

# Assuming you've already trained the model and have 'y_kmeans' as your cluster labels
silhouette_avg = silhouette_score(X_scaled, y_kmeans)
print("Silhouette Score for K-Means Clustering:", silhouette_avg)
